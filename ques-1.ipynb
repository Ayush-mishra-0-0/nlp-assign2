{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, \n",
    "    DistilBertForQuestionAnswering,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=384):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = self.preprocess_data(data)\n",
    "    \n",
    "    def preprocess_data(self, data):\n",
    "        examples = []\n",
    "        for entry in data['data']:\n",
    "            for paragraph in entry['paragraphs']:\n",
    "                context = paragraph['context']\n",
    "                for qa in paragraph['qas']:\n",
    "                    if not qa['is_impossible']:\n",
    "                        answer = qa['answers'][0]\n",
    "                        # Calculate answer end position properly\n",
    "                        answer_start = answer['answer_start']\n",
    "                        answer_text = answer['text']\n",
    "                        answer_end = answer_start + len(answer_text)\n",
    "                        \n",
    "                        examples.append({\n",
    "                            'context': context,\n",
    "                            'question': qa['question'],\n",
    "                            'answer_text': answer_text,\n",
    "                            'answer_start': answer_start,\n",
    "                            'answer_end': answer_end\n",
    "                        })\n",
    "        return examples[:10000]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.examples[idx]\n",
    "        \n",
    "        # Tokenize question and context together\n",
    "        encoding = self.tokenizer(\n",
    "            example['question'],\n",
    "            example['context'],\n",
    "            max_length=self.max_length,\n",
    "            truncation='only_second',  # Only truncate the context, not the question\n",
    "            stride=128,  # Add sliding window for long contexts\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_offsets_mapping=True,  # Get character mappings\n",
    "            return_overflowing_tokens=True  # Handle long sequences properly\n",
    "        )\n",
    "        \n",
    "        # Convert char positions to token positions\n",
    "        offset_mapping = encoding.pop('offset_mapping').squeeze()\n",
    "        \n",
    "        # Find the token positions that correspond to the answer\n",
    "        start_positions = torch.tensor([0])  # Default to 0 if answer not found\n",
    "        end_positions = torch.tensor([0])\n",
    "        \n",
    "        for idx, (start, end) in enumerate(offset_mapping):\n",
    "            if start <= example['answer_start'] <= end:\n",
    "                start_positions = torch.tensor([idx])\n",
    "            if start <= example['answer_end'] <= end:\n",
    "                end_positions = torch.tensor([idx])\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, model_name='distilbert-base-uncased', device='cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertForQuestionAnswering.from_pretrained(\n",
    "            model_name,\n",
    "            return_dict=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Suppress specific warnings\n",
    "        warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "    \n",
    "    def train(self, train_dataloader, eval_dataloader, epochs=3, lr=2e-5):\n",
    "        # Use PyTorch's native AdamW implementation\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)\n",
    "        \n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=total_steps // 10,  # 10% warmup\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        training_stats = []\n",
    "        best_eval_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            progress_bar = tqdm(train_dataloader, desc=f'Training')\n",
    "            for batch in progress_bar:\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Backward pass with gradient clipping\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "            eval_loss = self.evaluate(eval_dataloader)\n",
    "            \n",
    "            # Save best model\n",
    "            if eval_loss < best_eval_loss:\n",
    "                best_eval_loss = eval_loss\n",
    "                torch.save(self.model.state_dict(), 'best_model.pt')\n",
    "            \n",
    "            training_stats.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'training_loss': avg_train_loss,\n",
    "                'eval_loss': eval_loss,\n",
    "                'learning_rate': scheduler.get_last_lr()[0]\n",
    "            })\n",
    "            \n",
    "            print(f'Average training loss: {avg_train_loss:.4f}')\n",
    "            print(f'Evaluation loss: {eval_loss:.4f}')\n",
    "        \n",
    "        return training_stats\n",
    "    \n",
    "    def evaluate(self, eval_dataloader):\n",
    "        self.model.eval()\n",
    "        total_eval_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, desc='Evaluating'):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                total_eval_loss += outputs.loss.item()\n",
    "        \n",
    "        return total_eval_loss / len(eval_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(attention_weights, layer_idx=0, head_idx=0):\n",
    "    \"\"\"Plot attention heatmap for a specific layer and head\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attention_weights[layer_idx][0, head_idx].cpu().numpy(),\n",
    "        cmap='Blues'\n",
    "    )\n",
    "    plt.title(f'Attention Heatmap - Layer {layer_idx}, Head {head_idx}')\n",
    "    plt.xlabel('Key tokens')\n",
    "    plt.ylabel('Query tokens')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 371/371 [02:23<00:00,  2.58it/s] \n",
      "  with amp.autocast(enabled=torch.cuda.is_available()):\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 371/371 [02:23<00:00,  2.58it/s] \n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 371/371 [02:23<00:00,  2.58it/s] \n",
      "Average training loss: 3.3860\n",
      "Average training loss: 3.3860\n",
      "Evaluation loss: 2.3685\n",
      "Evaluation loss: 2.3685\n",
      "\n",
      "Epoch 2/3\n",
      "Epoch 2/3\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [22:28<00:00,  2.16s/it, loss=1.8109] \n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 371/371 [02:11<00:00,  2.82it/s] \n",
      "Average training loss: 1.8006\n",
      "Evaluation loss: 2.1851\n",
      "\n",
      "Epoch 3/3\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [24:56<00:00,  2.39s/it, loss=1.4325] \n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 371/371 [02:10<00:00,  2.83it/s] \n",
      "Average training loss: 1.4130\n",
      "Evaluation loss: 2.1851\n",
      "\n",
      "Epoch 3/3\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [24:56<00:00,  2.39s/it, loss=1.4325] \n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 371/371 [02:10<00:00,  2.83it/s] \n",
      "Average training loss: 1.4130\n",
      "Epoch 3/3\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 625/625 [24:56<00:00,  2.39s/it, loss=1.4325] \n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 371/371 [02:10<00:00,  2.83it/s] \n",
      "Average training loss: 1.4130\n",
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 371/371 [02:10<00:00,  2.83it/s] \n",
      "Average training loss: 1.4130\n",
      "Average training loss: 1.4130\n",
      "Evaluation loss: 2.1718\n",
      "Training complete! Results saved in 'results' directory.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Load data\n",
    "    with open('train-v2.0.json', 'r') as f:\n",
    "        train_data = json.load(f)\n",
    "    with open('dev-v2.0.json', 'r') as f:\n",
    "        eval_data = json.load(f)\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = ModelTrainer()\n",
    "    \n",
    "    # Create datasets with larger batch size and num_workers\n",
    "    train_dataset = QADataset(train_data, trainer.tokenizer)\n",
    "    eval_dataset = QADataset(eval_data, trainer.tokenizer)\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=32,  # Increased batch size\n",
    "        shuffle=True,\n",
    "        num_workers=4,  # Parallel data loading\n",
    "        pin_memory=True  # Faster data transfer to GPU\n",
    "    )\n",
    "    \n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset, \n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    training_stats = trainer.train(train_dataloader, eval_dataloader)\n",
    "    \n",
    "    # Plot training results\n",
    "    stats_df = pd.DataFrame(training_stats)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(stats_df['epoch'], stats_df['training_loss'], label='Training Loss')\n",
    "    plt.plot(stats_df['epoch'], stats_df['eval_loss'], label='Evaluation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Evaluation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ayush\\apython\\envs\\gpu\\Lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Ayush\\AppData\\Local\\Temp\\ipykernel_12880\\3608060869.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who created Python?\n",
      "Answer: Guido van Rossum\n",
      "Confidence: 86.35%\n",
      "Answer span: characters 47 to 63\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForQuestionAnswering\n",
    "\n",
    "class QAInference:\n",
    "    def __init__(self, model_path='results/best_model.pt', model_name='distilbert-base-uncased'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = DistilBertForQuestionAnswering.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        # Load the trained model weights\n",
    "        checkpoint = torch.load(model_path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_answer(self, question, context, max_length=384):\n",
    "        # Tokenize input\n",
    "        inputs = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            max_length=max_length,\n",
    "            truncation='only_second',\n",
    "            stride=128,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Get offset mapping and send input_ids and attention_mask to device\n",
    "        offset_mapping = inputs.pop('offset_mapping').squeeze(0)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = self.model(**inputs)\n",
    "        \n",
    "        # Get the most likely beginning and end of answer\n",
    "        start_logits = outputs.start_logits.cpu().squeeze(0)\n",
    "        end_logits = outputs.end_logits.cpu().squeeze(0)\n",
    "        \n",
    "        # Get the most likely answer span\n",
    "        start_idx = torch.argmax(start_logits)\n",
    "        end_idx = torch.argmax(end_logits)\n",
    "        \n",
    "        # Convert to actual text span using offset mapping\n",
    "        start_char = int(offset_mapping[start_idx][0])\n",
    "        end_char = int(offset_mapping[end_idx][1])\n",
    "        \n",
    "        # Get the answer text\n",
    "        answer = context[start_char:end_char]\n",
    "        \n",
    "        # Calculate confidence scores\n",
    "        start_prob = torch.softmax(start_logits, dim=0)[start_idx].item()\n",
    "        end_prob = torch.softmax(end_logits, dim=0)[end_idx].item()\n",
    "        confidence = (start_prob + end_prob) / 2\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'confidence': confidence,\n",
    "            'start_char': start_char,\n",
    "            'end_char': end_char\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the QA system\n",
    "    qa_system = QAInference()\n",
    "    \n",
    "    # Example context and question\n",
    "    context = \"The Python programming language was created by Guido van Rossum and was released in 1991. Python's name comes from Monty Python.\"\n",
    "    question = \"Who created Python?\"\n",
    "    \n",
    "    # Get the answer\n",
    "    result = qa_system.get_answer(question, context)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"Answer span: characters {result['start_char']} to {result['end_char']}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
