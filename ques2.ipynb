{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionQueryDataset(Dataset):\n",
    "    def __init__(self, questions, queries, tokenizer):\n",
    "        self.questions = questions\n",
    "        self.queries = queries\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        query = self.queries[idx]\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        question_tokens = self.tokenizer.encode(\n",
    "            question, \n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        query_tokens = self.tokenizer.encode(\n",
    "            query,\n",
    "            add_special_tokens=True,\n",
    "            max_length=256,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'question': question_tokens.squeeze(),\n",
    "            'query': query_tokens.squeeze(),\n",
    "            'question_text': question,\n",
    "            'query_text': query\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # Pack padded sequence\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded,\n",
    "            lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(packed)\n",
    "        \n",
    "        # Unpack outputs\n",
    "        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        # Combine bidirectional states\n",
    "        hidden = self._combine_bidirectional_states(hidden)\n",
    "        cell = self._combine_bidirectional_states(cell)\n",
    "        \n",
    "        return outputs, (hidden, cell)\n",
    "    \n",
    "    def _combine_bidirectional_states(self, state):\n",
    "        \"\"\"Combines forward and backward states for bidirectional LSTM\"\"\"\n",
    "        # state shape: (num_layers * 2, batch_size, hidden_size)\n",
    "        forward_state = state[::2]\n",
    "        backward_state = state[1::2]\n",
    "        combined_state = torch.cat([forward_state, backward_state], dim=2)\n",
    "        return combined_state  # shape: (num_layers, batch_size, hidden_size * 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate attention scores\n",
    "        # hidden shape: (batch_size, hidden_size * 2)\n",
    "        # encoder_outputs shape: (batch_size, seq_len, hidden_size * 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attention(encoder_outputs))  # (batch_size, seq_len, hidden_size)\n",
    "        energy = energy.transpose(1, 2)  # (batch_size, hidden_size, seq_len)\n",
    "        \n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # (batch_size, 1, hidden_size)\n",
    "        attention = torch.bmm(v, energy)  # (batch_size, 1, seq_len)\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        return torch.softmax(attention, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        \n",
    "        # Modified LSTM input size to account for concatenated context vector\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size + hidden_size * 2,  # input size: embedding + context\n",
    "            hidden_size * 2,  # hidden size matches encoder's bidirectional output\n",
    "            num_layers=num_layers,  # match encoder's number of layers\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        # x shape: (batch_size, 1)\n",
    "        x = x.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(x))  # (batch_size, 1, embed_size)\n",
    "        \n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        # Apply attention to encoder outputs\n",
    "        context = torch.bmm(attention_weights, encoder_outputs)\n",
    "        \n",
    "        # Combine embedded input and context vector\n",
    "        rnn_input = torch.cat([embedded, context], dim=2)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        # No need to unsqueeze hidden and cell as they're already the right shape\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        \n",
    "        # Generate prediction\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        \n",
    "        return prediction, hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Seq2Seq model\n",
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.size(0)\n",
    "        target_len = target.size(1)\n",
    "        target_vocab_size = self.decoder.fc.out_features\n",
    "        \n",
    "        # Initialize outputs tensor\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
    "        \n",
    "        # Get encoder outputs\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(\n",
    "            source,\n",
    "            torch.tensor([source.size(1)] * batch_size)\n",
    "        )\n",
    "        \n",
    "        # First input to decoder is start token\n",
    "        decoder_input = target[:, 0]\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(\n",
    "                decoder_input,\n",
    "                hidden,\n",
    "                cell,\n",
    "                encoder_outputs\n",
    "            )\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            decoder_input = target[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity and relation linking\n",
    "class EntityLinker:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        \n",
    "    def link_entities(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            entities.append({\n",
    "                'text': ent.text,\n",
    "                'label': ent.label_,\n",
    "                'start': ent.start_char,\n",
    "                'end': ent.end_char\n",
    "            })\n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        source = batch['question'].to(device)\n",
    "        target = batch['query'].to(device)\n",
    "        \n",
    "        output = model(source, target)\n",
    "        output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "        target = target[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct_queries = 0\n",
    "    total_queries = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            source = batch['question'].to(device)\n",
    "            target = batch['query'].to(device)\n",
    "            \n",
    "            output = model(source, target, teacher_forcing_ratio=0)\n",
    "            output = output[:, 1:].reshape(-1, output.shape[-1])\n",
    "            target = target[:, 1:].reshape(-1)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = output.argmax(1)\n",
    "            correct = (predictions == target).sum().item()\n",
    "            total = target.size(0)\n",
    "            \n",
    "            correct_queries += correct\n",
    "            total_queries += total\n",
    "            \n",
    "    accuracy = correct_queries / total_queries\n",
    "    return epoch_loss / len(test_loader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # Load and preprocess data\n",
    "#     with open('qald_9_plus_train_wikidata.json', 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     # Extract questions and queries\n",
    "#     questions = []\n",
    "#     queries = []\n",
    "#     for item in data['questions']:\n",
    "#         en_question = next(\n",
    "#             (q['string'] for q in item['question'] if q['language'] == 'en'),\n",
    "#             None\n",
    "#         )\n",
    "#         sparql_query = item.get('query', {}).get('sparql', '')\n",
    "        \n",
    "#         if en_question and sparql_query:\n",
    "#             questions.append(en_question)\n",
    "#             queries.append(sparql_query)\n",
    "    \n",
    "#     # Split data\n",
    "#     train_questions, test_questions, train_queries, test_queries = train_test_split(\n",
    "#         questions, queries, test_size=0.2, random_state=42\n",
    "#     )\n",
    "    \n",
    "#     # Initialize tokenizer and dataset\n",
    "#     tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "#     train_dataset = QuestionQueryDataset(train_questions, train_queries, tokenizer)\n",
    "#     test_dataset = QuestionQueryDataset(test_questions, test_queries, tokenizer)\n",
    "    \n",
    "#     # Create data loaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "#     # Initialize model components\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "#     encoder = Encoder(\n",
    "#         vocab_size=tokenizer.vocab_size,\n",
    "#         embed_size=256,\n",
    "#         hidden_size=512,\n",
    "#         num_layers=2\n",
    "#     )\n",
    "    \n",
    "#     decoder = Decoder(\n",
    "#         vocab_size=tokenizer.vocab_size,\n",
    "#         embed_size=256,\n",
    "#         hidden_size=512,\n",
    "#         num_layers=2\n",
    "#     )\n",
    "    \n",
    "#     model = Seq2SeqModel(encoder, decoder, device).to(device)\n",
    "    \n",
    "#     # Initialize optimizer and criterion\n",
    "#     optimizer = torch.optim.Adam(model.parameters())\n",
    "#     criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "#     # Training loop\n",
    "#     n_epochs = 10\n",
    "#     best_accuracy = 0\n",
    "    \n",
    "#     for epoch in range(n_epochs):\n",
    "#         train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "#         test_loss, accuracy = evaluate(model, test_loader, criterion, device)\n",
    "        \n",
    "#         print(f'Epoch: {epoch+1}')\n",
    "#         print(f'Train Loss: {train_loss:.4f}')\n",
    "#         print(f'Test Loss: {test_loss:.4f}')\n",
    "#         print(f'Query Generation Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "#         if accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the capital of France?\n",
      "Generated SPARQL Query: SELECT ?capital WHERE {\n",
      "  wd:Q142 wdt:P36 ?capital .\n",
      "}\n",
      "\n",
      "Question: Who is the president of the United States?\n",
      "Generated SPARQL Query: SELECT ?president WHERE {\n",
      "  wd:Q30 wdt:P6 ?president .\n",
      "}\n",
      "Question: What is the population of Tokyo?\n",
      "Generated SPARQL Query: SELECT ?population WHERE {\n",
      "  wd:Q1490 wdt:P1082 ?population .\n",
      "}\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def prepare_model(model_path):\n",
    "    \"\"\"\n",
    "    Load and prepare the trained model for inference\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Initialize model architecture (same as training)\n",
    "    encoder = Encoder(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_size=256,\n",
    "        hidden_size=512,\n",
    "        num_layers=2\n",
    "    )\n",
    "    \n",
    "    decoder = Decoder(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        embed_size=256,\n",
    "        hidden_size=512,\n",
    "        num_layers=2\n",
    "    )\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = Seq2SeqModel(encoder, decoder, device).to(device)\n",
    "    \n",
    "    # Load trained weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "def generate_query(question, model, tokenizer, device, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate a SPARQL query for a given question using the trained model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the input question\n",
    "        question_tokens = tokenizer.encode(\n",
    "            question,\n",
    "            add_special_tokens=True,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "        \n",
    "        # Initialize output with SOS token\n",
    "        output = torch.tensor([[tokenizer.cls_token_id]]).to(device)\n",
    "        \n",
    "        # Get initial encoder output\n",
    "        encoder_outputs, (hidden, cell) = model.encoder(\n",
    "            question_tokens,\n",
    "            torch.tensor([question_tokens.size(1)])\n",
    "        )\n",
    "        \n",
    "        # Generate query token by token\n",
    "        for _ in range(max_length):\n",
    "            # Get decoder output\n",
    "            predictions, hidden, cell = model.decoder(\n",
    "                output[:, -1],\n",
    "                hidden,\n",
    "                cell,\n",
    "                encoder_outputs\n",
    "            )\n",
    "            \n",
    "            # Get most likely next token\n",
    "            next_token = predictions.argmax(1).unsqueeze(1)\n",
    "            output = torch.cat([output, next_token], dim=1)\n",
    "            \n",
    "            # Stop if EOS token is generated\n",
    "            if next_token.item() == tokenizer.sep_token_id:\n",
    "                break\n",
    "        \n",
    "        # Decode the generated query\n",
    "        generated_query = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        return generated_query\n",
    "\n",
    "def predict_sparql(question, model_path):\n",
    "    \"\"\"\n",
    "    Main function to predict SPARQL query for a new question\n",
    "    \"\"\"\n",
    "    # Prepare model and tokenizer\n",
    "    model, tokenizer, device = prepare_model(model_path)\n",
    "    \n",
    "    # Generate query\n",
    "    sparql_query = generate_query(question, model, tokenizer, device)\n",
    "    \n",
    "    return sparql_query\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"best_model.pt\"\n",
    "    \n",
    "    # Example questions\n",
    "    test_questions = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Who is the president of the United States?\",\n",
    "        \"What is the population of Tokyo?\"\n",
    "    ]\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(\"\\nQuestion:\", question)\n",
    "        sparql_query = predict_sparql(question, model_path)\n",
    "        print(\"Generated SPARQL Query:\", sparql_query)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
